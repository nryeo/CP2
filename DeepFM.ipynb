{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stebechoi/CP2/blob/YJ/DeepFM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FyIoDYr912rt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from itertools import repeat\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Input, Embedding, Concatenate, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iBPbLRS7bC1",
        "outputId": "c646fff4-67cc-43eb-9452-8a3e31486232"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 불러오기"
      ],
      "metadata": {
        "id": "edjf_YbGRCij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/CP2/ml-100k/'\n",
        "ratings_df = pd.read_csv(data_path + 'u.data', sep='\\t', names=['userId', 'movieId', 'rating', 'timestamp'])\n",
        "genre_data = pd.read_csv(data_path + 'u.genre', sep='|', names=['genre', 'genre_id'])\n",
        "item_df = pd.read_csv(data_path + 'u.item', sep='|', encoding='latin-1', header=None,\n",
        "                        names=['movieId', 'movie_title', 'release_date', 'video_release_date',\n",
        "                               'IMDb_URL', 'unknown', 'Action', 'Adventure', 'Animation',\n",
        "                               'Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama',\n",
        "                               'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery',\n",
        "                               'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'])\n",
        "users_df = pd.read_csv(data_path + 'u.user', sep='|', names = ['userId', 'age', 'gender', 'occupation', 'zip_code'])\n",
        "\n",
        "ratings_df = ratings_df.drop('timestamp',axis=1)\n",
        "users_df = users_df.drop('zip_code',axis=1)\n",
        "item_df = item_df.drop(['unknown','movie_title', 'release_date', 'video_release_date', 'IMDb_URL'], axis=1)"
      ],
      "metadata": {
        "id": "Du-frSfc186o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Vector"
      ],
      "metadata": {
        "id": "t0WzjN6ZRGSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#gender\n",
        "genders_df = pd.get_dummies(users_df.gender, prefix=\"gender\")\n",
        "users_df = pd.concat([users_df, genders_df], axis=1)\n",
        "users_df.drop(\"gender\", axis=1, inplace=True)\n",
        "\n",
        "def convert_age(x):\n",
        "  if x < 18:\n",
        "    return 'under 18'\n",
        "  elif x>= 18 and x<25:\n",
        "    return '18-24'\n",
        "  elif x>=25 and x<35:\n",
        "    return '25-34'\n",
        "  elif x>=35 and x<45:\n",
        "    return '35-44'\n",
        "  elif x>=45 and x<55:\n",
        "    return '45-54'\n",
        "  else:\n",
        "    return 'over 55'\n",
        "\n",
        "#genres\n",
        "genres_df = item_df.iloc[:,1:]\n",
        "\n",
        "#age \n",
        "users_df.age = users_df.age.apply(convert_age)\n",
        "ages_df = pd.get_dummies(users_df.age)\n",
        "users_df = pd.concat([users_df, ages_df], axis=1)\n",
        "users_df.drop('age', axis=1,inplace=True)\n",
        "\n",
        "#occupation\n",
        "occupation_df = pd.get_dummies(users_df.occupation)\n",
        "users_df = pd.concat([users_df, occupation_df], axis=1)\n",
        "users_df.drop('occupation',axis=1, inplace=True)\n",
        "\n",
        "ratings_df = ratings_df.merge(users_df, how=\"left\")\n",
        "ratings_df = ratings_df.merge(item_df, how='left')\n",
        "ratings_df = ratings_df.astype(float)"
      ],
      "metadata": {
        "id": "RwjlWagY2EBK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 타겟"
      ],
      "metadata": {
        "id": "AE4JhnPiRQaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#binary target\n",
        "target = ratings_df['rating']\n",
        "binary_target = (target>=4.0).astype(float)\n",
        "ratings_df.drop('rating',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "uHFLzEPgRP1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## \bLook up table"
      ],
      "metadata": {
        "id": "sDyqD4WGRVSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fields\n",
        "fields = [ratings_df.columns[i] for i in range(ratings_df.shape[1])]\n",
        "num_fields = len(fields)\n",
        "\n",
        "field_name = {\"userId\": [\"userId\"],\n",
        "              \"movieId\": [\"movieId\"],\n",
        "              \"gender\": list(genders_df.columns),\n",
        "              \"age\": list(ages_df.columns),\n",
        "              \"occupation\": list(occupation_df.columns),\n",
        "              \"genres\": list(genres_df.columns)}\n",
        "\n",
        "#embedding lookup index\n",
        "field_dict = dict()\n",
        "embedding_lookup_index = []\n",
        "for index, field in enumerate(list(field_name.keys())):\n",
        "  field_dict[index] = field\n",
        "  embedding_lookup_index.extend(repeat(index, len(field_name[field])))"
      ],
      "metadata": {
        "id": "KF5yfpXRRZ5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FM Part"
      ],
      "metadata": {
        "id": "AUBQi8jKRemR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FM part\n",
        "class wide_part(keras.layers.Layer):\n",
        "    def __init__(self, num_feature, num_fields,embedding_size, embedding_lookup_index):\n",
        "        super(wide_part,self).__init__()\n",
        "        self.num_fields = num_fields\n",
        "        self.embedding_lookup_index = embedding_lookup_index\n",
        "        self.w = tf.Variable(tf.random.normal(shape=[num_feature],\n",
        "                                              mean=0.0, stddev=1.0), name='w')\n",
        "        self.V = tf.Variable(tf.random.normal(shape=(num_fields, embedding_size),\n",
        "                                              mean=0.0, stddev=0.01), name='V')\n",
        "    \n",
        "\n",
        "    def call(self, inputs):\n",
        "        #embeds와 (batch_size, num_feature, embedding_size) - feature tensor\n",
        "        x_batch = keras.layers.Reshape((inputs.shape[-1], 1))(inputs)\n",
        "        # 인덱스에 해당하는 임베딩 벡터를 찾는 과정에서 field_index라능 2차원 벡터를 사용하여 3차원 텐서가 생성됨\n",
        "        embeddings_lookup_table = tf.nn.embedding_lookup(params=self.V, ids=self.embedding_lookup_index)\n",
        "        # (50,V) --> embedding_lookup_table\n",
        "        # x_batch, embeds broadcasting (vx)\n",
        "        embedded_fields = tf.math.multiply(x_batch, embeddings_lookup_table)\n",
        "        # element-wise after broadcasting to (None,50,1) --> (None,50,V)\n",
        "\n",
        "        order_1_output = tf.reduce_sum(tf.math.multiply(inputs, self.w), axis=1)\n",
        "        #         elementwise after broadcasting (None,50) x (50) = None,50\n",
        "        #         reduce_sum == (None,)\n",
        "\n",
        "        embed_sum = tf.reduce_sum(embedded_fields, [1, 2])\n",
        "        # (None,50,V) == > (None,)\n",
        "        embed_square = tf.square(embedded_fields)\n",
        "        # (None,50,V) ==> (None,50,V)\n",
        "        square_of_sum = tf.square(embed_sum)\n",
        "        # (None,) == > (None,)\n",
        "        sum_of_square = tf.reduce_sum(embed_square, [1, 2])\n",
        "        # (None,50,V) == > (None, )\n",
        "        order_2_output = 0.5 * tf.subtract(square_of_sum, sum_of_square)\n",
        "        # (None,) ==> (None,)\n",
        "        order_1_output = keras.layers.Reshape([1])(order_1_output)\n",
        "        # (None,) ==> (None,1)\n",
        "        order_2_output = keras.layers.Reshape([1])(order_2_output)\n",
        "        # (None,) ==> (None,1)\n",
        "        wide_output = keras.layers.Concatenate(axis=1)([order_1_output, order_2_output])\n",
        "        # (None,2)\n",
        "\n",
        "        return wide_output, embedded_fields"
      ],
      "metadata": {
        "id": "uedUv1mwaMyn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DeepFM"
      ],
      "metadata": {
        "id": "GFk97LsXRiJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DeepFM\n",
        "class deep_FM(keras.Model):\n",
        "    def __init__(self, num_feature, num_fields, embedding_lookup_index, embedding_size):\n",
        "        super(deep_FM,self).__init__()\n",
        "        self.embedding_size = embedding_size      # k: 임베딩 벡터의 차원(크기)\n",
        "        self.num_feature = num_feature            # f: 원래 feature 개수\n",
        "        self.num_fields = num_fields              # m: grouped field 개수\n",
        "        self.embedding_lookup_index = embedding_lookup_index \n",
        "\n",
        "        self.fm_layer = wide_part(num_feature, num_fields, embedding_size, embedding_lookup_index)\n",
        "\n",
        "        self.layers1 = tf.keras.layers.Dense(units=64, activation='relu')\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate=0.2)\n",
        "        self.layers2 = tf.keras.layers.Dense(units=16, activation='relu')\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate=0.2)\n",
        "        self.layers3 = tf.keras.layers.Dense(units=2, activation='relu')\n",
        "\n",
        "        self.final = tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs = (None,50)\n",
        "        wide_output, embedded_fields = self.fm_layer(inputs)\n",
        "\n",
        "        # retrieve Dense Vectors: (num_batch, num_feature*embedding_size)\n",
        "        embedded_fields = tf.reshape(embedded_fields, [-1, self.num_feature*self.embedding_size])\n",
        "\n",
        "        # 2) Deep Component\n",
        "        y_deep = self.layers1(embedded_fields)\n",
        "        y_deep = self.dropout1(y_deep)\n",
        "        y_deep = self.layers2(y_deep)\n",
        "        y_deep = self.dropout2(y_deep)\n",
        "        y_deep = self.layers3(y_deep)\n",
        "\n",
        "        # Concatenation\n",
        "        y_pred = tf.concat([wide_output, y_deep], 1)\n",
        "        y_pred = self.final(y_pred)\n",
        "        #[batchsize,1] 에서 [baichsize] 로 차원을 변경\n",
        "        y_pred = tf.reshape(y_pred, [-1, ])\n",
        "\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "YgxJ4CFabKMA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(ratings_df,binary_target,test_size= 0.2)"
      ],
      "metadata": {
        "id": "N4pmUMxAQ3wV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = deep_FM(num_feature=len(embedding_lookup_index),num_fields = len(field_dict), embedding_size = 5, embedding_lookup_index=embedding_lookup_index)"
      ],
      "metadata": {
        "id": "bCjTdbrMQQh5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision = tf.keras.metrics.Precision(top_k =5)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[precision])"
      ],
      "metadata": {
        "id": "fWOq1xQfq_2_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrgjqksGyiUg",
        "outputId": "18b797ab-4029-479b-f79b-995f01d6f728"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 5s 2ms/step - loss: 8.3873 - precision: 0.5150 - val_loss: 8.4834 - val_precision: 0.5891\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 2s 2ms/step - loss: 8.4597 - precision: 0.5877 - val_loss: 8.3494 - val_precision: 0.6185\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 2s 2ms/step - loss: 8.3843 - precision: 0.5542 - val_loss: 8.4810 - val_precision: 0.5911\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 8.4411 - precision: 0.5872 - val_loss: 8.2357 - val_precision: 0.5962\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 2s 2ms/step - loss: 8.3292 - precision: 0.5262 - val_loss: 8.3874 - val_precision: 0.5367\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 2s 2ms/step - loss: 8.3426 - precision: 0.5290 - val_loss: 7.8616 - val_precision: 0.5444\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 3s 2ms/step - loss: 8.4290 - precision: 0.5418 - val_loss: 8.5643 - val_precision: 0.5674\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 3s 2ms/step - loss: 8.5292 - precision: 0.5787 - val_loss: 8.5570 - val_precision: 0.5725\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 3s 3ms/step - loss: 8.3126 - precision: 0.5622 - val_loss: 8.5576 - val_precision: 0.5623\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 3s 2ms/step - loss: 8.5215 - precision: 0.5573 - val_loss: 8.5506 - val_precision: 0.5706\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4d4aa7f340>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}