{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stebechoi/CP2/blob/YJ/DeepFM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FyIoDYr912rt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from itertools import repeat\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Input, Embedding, Concatenate, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iBPbLRS7bC1",
        "outputId": "cc36e892-3f4c-4897-c915-489bc0bc68e7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/CP2/ml-100k/'\n",
        "ratings_df = pd.read_csv(data_path + 'u.data', sep='\\t', names=['userId', 'movieId', 'rating', 'timestamp'])\n",
        "genre_data = pd.read_csv(data_path + 'u.genre', sep='|', names=['genre', 'genre_id'])\n",
        "item_df = pd.read_csv(data_path + 'u.item', sep='|', encoding='latin-1', header=None,\n",
        "                        names=['movieId', 'movie_title', 'release_date', 'video_release_date',\n",
        "                               'IMDb_URL', 'unknown', 'Action', 'Adventure', 'Animation',\n",
        "                               'Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama',\n",
        "                               'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery',\n",
        "                               'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'])\n",
        "users_df = pd.read_csv(data_path + 'u.user', sep='|', names = ['userId', 'age', 'gender', 'occupation', 'zip_code'])\n",
        "\n",
        "ratings_df = ratings_df.drop('timestamp',axis=1)\n",
        "users_df = users_df.drop('zip_code',axis=1)\n",
        "item_df = item_df.drop(['unknown','movie_title', 'release_date', 'video_release_date', 'IMDb_URL'], axis=1)"
      ],
      "metadata": {
        "id": "Du-frSfc186o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gender\n",
        "genders_df = pd.get_dummies(users_df.gender, prefix=\"gender\")\n",
        "users_df = pd.concat([users_df, genders_df], axis=1)\n",
        "users_df.drop(\"gender\", axis=1, inplace=True)\n",
        "\n",
        "def convert_age(x):\n",
        "  if x < 18:\n",
        "    return 'under 18'\n",
        "  elif x>= 18 and x<25:\n",
        "    return '18-24'\n",
        "  elif x>=25 and x<35:\n",
        "    return '25-34'\n",
        "  elif x>=35 and x<45:\n",
        "    return '35-44'\n",
        "  elif x>=45 and x<55:\n",
        "    return '45-54'\n",
        "  else:\n",
        "    return 'over 55'"
      ],
      "metadata": {
        "id": "RwjlWagY2EBK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#genres\n",
        "genres_df = item_df.iloc[:,1:]"
      ],
      "metadata": {
        "id": "45jntDBf70cZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#age \n",
        "users_df.age = users_df.age.apply(convert_age)\n",
        "ages_df = pd.get_dummies(users_df.age)\n",
        "users_df = pd.concat([users_df, ages_df], axis=1)\n",
        "users_df.drop('age', axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "a2OmbHSS2IF0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#occupation\n",
        "occupation_df = pd.get_dummies(users_df.occupation)\n",
        "users_df = pd.concat([users_df, occupation_df], axis=1)\n",
        "users_df.drop('occupation',axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "4yUHiStg2Iq6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_df = ratings_df.merge(users_df, how=\"left\")\n",
        "ratings_df = ratings_df.merge(item_df, how='left')\n",
        "ratings_df = ratings_df.astype(float)"
      ],
      "metadata": {
        "id": "KNT5mhpp2JL7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#binary target\n",
        "target = ratings_df['rating']\n",
        "binary_target = (target>=4.0).astype(float)\n",
        "ratings_df.drop('rating',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "tB-rWL-t2JPQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fields\n",
        "fields = [ratings_df.columns[i] for i in range(ratings_df.shape[1])]\n",
        "num_fields = len(fields)\n",
        "\n",
        "field_name = {\"userId\": [\"userId\"],\n",
        "              \"movieId\": [\"movieId\"],\n",
        "              \"gender\": list(genders_df.columns),\n",
        "              \"age\": list(ages_df.columns),\n",
        "              \"occupation\": list(occupation_df.columns),\n",
        "              \"genres\": list(genres_df.columns)}"
      ],
      "metadata": {
        "id": "_8QuI2ivH7cs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#embedding lookup index\n",
        "field_dict = dict()\n",
        "embedding_lookup_index = []\n",
        "for index, field in enumerate(list(field_name.keys())):\n",
        "  field_dict[index] = field\n",
        "  embedding_lookup_index.extend(repeat(index, len(field_name[field])))"
      ],
      "metadata": {
        "id": "2gVFHRt32JTr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #FM part\n",
        "# class wide_part(keras.layers.Layer):\n",
        "#     def __init__(self, V, num_fields, embedding_lookup_index, **kwargs):\n",
        "#         super().__init__(self, **kwargs)\n",
        "#         self.V = V\n",
        "#         self.num_fields = num_fields\n",
        "#         self.embedding_lookup_index = embedding_lookup_index\n",
        "\n",
        "#     def build(self, input_shape):\n",
        "#         w_init = tf.random_normal_initializer()\n",
        "\n",
        "#         self.W = tf.Variable(initial_value=w_init(shape=[input_shape[-1]]),\n",
        "#                              dtype='float32',name = \"W\")\n",
        "#         self.V = tf.Variable(initial_value=w_init(shape=[self.num_fields, self.V]),\n",
        "#                              dtype=\"float32\",name= \"V\")\n",
        "\n",
        "#     def call(self, inputs):\n",
        "#         #embeds와 (batch_size, num_feature, embedding_size) - feature tensor\n",
        "#         x_batch = keras.layers.Reshape((inputs.shape[-1], 1))(inputs)\n",
        "#         # 인덱스에 해당하는 임베딩 벡터를 찾는 과정에서 field_index라능 2차원 벡터를 사용하여 3차원 텐서가 생성됨\n",
        "#         embeddings_lookup_table = tf.nn.embedding_lookup(params=self.V, ids=self.embedding_lookup_index)\n",
        "#         # (50,V) --> embedding_lookup_table\n",
        "#         # x_batch, embeds broadcasting (vx)\n",
        "#         embedded_fields = tf.math.multiply(x_batch, embeddings_lookup_table)\n",
        "#         # element-wise after broadcasting to (None,50,1) --> (None,50,V)\n",
        "\n",
        "#         order_1_output = tf.reduce_sum(tf.math.multiply(inputs, self.W), axis=1)\n",
        "#         #         elementwise after broadcasting (None,50) x (50) = None,50\n",
        "#         #         reduce_sum == (None,)\n",
        "\n",
        "#         embed_sum = tf.reduce_sum(embedded_fields, [1, 2])\n",
        "#         # (None,50,V) == > (None,)\n",
        "#         embed_square = tf.square(embedded_fields)\n",
        "#         # (None,50,V) ==> (None,50,V)\n",
        "#         square_of_sum = tf.square(embed_sum)\n",
        "#         # (None,) == > (None,)\n",
        "#         sum_of_square = tf.reduce_sum(embed_square, [1, 2])\n",
        "#         # (None,50,V) == > (None, )\n",
        "#         order_2_output = 0.5 * tf.subtract(square_of_sum, sum_of_square)\n",
        "#         # (None,) ==> (None,)\n",
        "#         order_1_output = keras.layers.Reshape([1])(order_1_output)\n",
        "#         # (None,) ==> (None,1)\n",
        "#         order_2_output = keras.layers.Reshape([1])(order_2_output)\n",
        "#         # (None,) ==> (None,1)\n",
        "#         wide_output = keras.layers.Concatenate(axis=1)([order_1_output, order_2_output])\n",
        "#         # (None,2)\n",
        "\n",
        "#         return wide_output, embedded_fields\n"
      ],
      "metadata": {
        "id": "AXJjHIHU2JYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FM part\n",
        "class wide_part(keras.layers.Layer):\n",
        "    def __init__(self, num_feature, num_fields,embedding_size, embedding_lookup_index):\n",
        "        super(wide_part,self).__init__()\n",
        "        self.num_fields = num_fields\n",
        "        self.embedding_lookup_index = embedding_lookup_index\n",
        "        self.w = tf.Variable(tf.random.normal(shape=[num_feature],\n",
        "                                              mean=0.0, stddev=1.0), name='w')\n",
        "        self.V = tf.Variable(tf.random.normal(shape=(num_fields, embedding_size),\n",
        "                                              mean=0.0, stddev=0.01), name='V')\n",
        "    \n",
        "\n",
        "    def call(self, inputs):\n",
        "        #embeds와 (batch_size, num_feature, embedding_size) - feature tensor\n",
        "        x_batch = keras.layers.Reshape((inputs.shape[-1], 1))(inputs)\n",
        "        # 인덱스에 해당하는 임베딩 벡터를 찾는 과정에서 field_index라능 2차원 벡터를 사용하여 3차원 텐서가 생성됨\n",
        "        embeddings_lookup_table = tf.nn.embedding_lookup(params=self.V, ids=self.embedding_lookup_index)\n",
        "        # (50,V) --> embedding_lookup_table\n",
        "        # x_batch, embeds broadcasting (vx)\n",
        "        embedded_fields = tf.math.multiply(x_batch, embeddings_lookup_table)\n",
        "        # element-wise after broadcasting to (None,50,1) --> (None,50,V)\n",
        "\n",
        "        order_1_output = tf.reduce_sum(tf.math.multiply(inputs, self.w), axis=1)\n",
        "        #         elementwise after broadcasting (None,50) x (50) = None,50\n",
        "        #         reduce_sum == (None,)\n",
        "\n",
        "        embed_sum = tf.reduce_sum(embedded_fields, [1, 2])\n",
        "        # (None,50,V) == > (None,)\n",
        "        embed_square = tf.square(embedded_fields)\n",
        "        # (None,50,V) ==> (None,50,V)\n",
        "        square_of_sum = tf.square(embed_sum)\n",
        "        # (None,) == > (None,)\n",
        "        sum_of_square = tf.reduce_sum(embed_square, [1, 2])\n",
        "        # (None,50,V) == > (None, )\n",
        "        order_2_output = 0.5 * tf.subtract(square_of_sum, sum_of_square)\n",
        "        # (None,) ==> (None,)\n",
        "        order_1_output = keras.layers.Reshape([1])(order_1_output)\n",
        "        # (None,) ==> (None,1)\n",
        "        order_2_output = keras.layers.Reshape([1])(order_2_output)\n",
        "        # (None,) ==> (None,1)\n",
        "        wide_output = keras.layers.Concatenate(axis=1)([order_1_output, order_2_output])\n",
        "        # (None,2)\n",
        "\n",
        "        return wide_output, embedded_fields"
      ],
      "metadata": {
        "id": "uedUv1mwaMyn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Deep part\n",
        "# class deep_part(keras.layers.Layer):\n",
        "#     def __init__(self, layer_list=[128, 64, 32], dropout_rate=0.5, activation=\"relu\", **kwargs):\n",
        "#         super().__init__(**kwargs)\n",
        "#         self.activaiton_fn = keras.activations.get(activation)\n",
        "#         self.dropout_rate = dropout_rate\n",
        "#         self.dense_layer_list = [keras.layers.Dense(num_neuron, activation=self.activaiton_fn, name =f'Dense_{index}') for index,num_neuron in\n",
        "#                                  enumerate(layer_list)]\n",
        "#         self.output_layer = keras.layers.Dense(1, activation=\"relu\",name = \"deep_output\")\n",
        "\n",
        "#     def call(self, inputs):\n",
        "#         embed_2d = inputs\n",
        "#         # (None,50,V)\n",
        "#         embed_2d = keras.layers.Flatten(name='flat_embed')(embed_2d)\n",
        "#         # (None,50 * V)\n",
        "#         result = embed_2d\n",
        "#         for layer in self.dense_layer_list:\n",
        "#             result = keras.layers.Dropout(self.dropout_rate)(result)\n",
        "#             result = layer(result)\n",
        "\n",
        "#         deep_result = self.output_layer(result)\n",
        "#         #(None,1)\n",
        "#         return deep_result"
      ],
      "metadata": {
        "id": "r_ok1n8c2Jc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #DeepFM\n",
        "# class deep_FM(keras.Model):\n",
        "#     def __init__(self, V, num_fields, embbeding_lookup_index, layer_list=[128, 64, 32], dropout_rate=0.5,\n",
        "#                  activation=\"relu\"):\n",
        "#         super().__init__(**kwargs)\n",
        "#         self.wide_part = wide_part(V, num_fields, embbeding_lookup_index)\n",
        "#         self.deep_part = deep_part(layer_list, dropout_rate, activation)\n",
        "#         self.output_layer = keras.layers.Dense(1, activation=\"sigmoid\",name = \"final_output\")\n",
        "\n",
        "#     def call(self, inputs):\n",
        "#         # inputs = (None,50)\n",
        "#         wide_output, embeddings = self.wide_part(inputs)\n",
        "#         deep_output = self.deep_part(embeddings)\n",
        "\n",
        "#         concat = keras.layers.Concatenate(axis=1)([wide_output, deep_output])\n",
        "#         wide_deep_output = self.output_layer(concat)\n",
        "#         return wide_deep_output"
      ],
      "metadata": {
        "id": "62zQ0fo0P2ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DeepFM\n",
        "class deep_FM(keras.Model):\n",
        "    def __init__(self, num_feature, num_fields, embedding_lookup_index, embedding_size):\n",
        "        super(deep_FM,self).__init__()\n",
        "        self.embedding_size = embedding_size      # k: 임베딩 벡터의 차원(크기)\n",
        "        self.num_feature = num_feature            # f: 원래 feature 개수\n",
        "        self.num_fields = num_fields              # m: grouped field 개수\n",
        "        self.embedding_lookup_index = embedding_lookup_index \n",
        "\n",
        "        self.fm_layer = wide_part(num_feature, num_fields, embedding_size, embedding_lookup_index)\n",
        "\n",
        "        self.layers1 = tf.keras.layers.Dense(units=64, activation='relu')\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate=0.2)\n",
        "        self.layers2 = tf.keras.layers.Dense(units=16, activation='relu')\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate=0.2)\n",
        "        self.layers3 = tf.keras.layers.Dense(units=2, activation='relu')\n",
        "\n",
        "        self.final = tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs = (None,50)\n",
        "        wide_output, embedded_fields = self.fm_layer(inputs)\n",
        "\n",
        "        # retrieve Dense Vectors: (num_batch, num_feature*embedding_size)\n",
        "        embedded_fields = tf.reshape(embedded_fields, [-1, self.num_feature*self.embedding_size])\n",
        "\n",
        "        # 2) Deep Component\n",
        "        y_deep = self.layers1(embedded_fields)\n",
        "        y_deep = self.dropout1(y_deep)\n",
        "        y_deep = self.layers2(y_deep)\n",
        "        y_deep = self.dropout2(y_deep)\n",
        "        y_deep = self.layers3(y_deep)\n",
        "\n",
        "        # Concatenation\n",
        "        y_pred = tf.concat([wide_output, y_deep], 1)\n",
        "        y_pred = self.final(y_pred)\n",
        "        #[batchsize,1] 에서 [baichsize] 로 차원을 변경\n",
        "        y_pred = tf.reshape(y_pred, [-1, ])\n",
        "\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "YgxJ4CFabKMA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(ratings_df,binary_target,test_size= 0.2)"
      ],
      "metadata": {
        "id": "N4pmUMxAQ3wV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = deep_FM(num_feature=len(embedding_lookup_index),num_fields = len(field_dict), embedding_size = 5, embedding_lookup_index=embedding_lookup_index)"
      ],
      "metadata": {
        "id": "bCjTdbrMQQh5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision = tf.keras.metrics.Precision(top_k =5)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[precision])"
      ],
      "metadata": {
        "id": "fWOq1xQfq_2_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrgjqksGyiUg",
        "outputId": "113c4f90-7fc5-45fe-d022-d39f8891a8ae"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 8s 4ms/step - loss: 6.0159 - precision: 0.5496 - val_loss: 1.2094 - val_precision: 0.6179\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.8235 - precision: 0.6440 - val_loss: 0.6622 - val_precision: 0.7214\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.6733 - precision: 0.7045 - val_loss: 0.6624 - val_precision: 0.7240\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.6685 - precision: 0.7126 - val_loss: 0.6567 - val_precision: 0.7450\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.6650 - precision: 0.7238 - val_loss: 0.6601 - val_precision: 0.7316\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.6603 - precision: 0.7328 - val_loss: 0.6575 - val_precision: 0.7438\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 4s 4ms/step - loss: 0.6579 - precision: 0.7346 - val_loss: 0.6510 - val_precision: 0.7514\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.6549 - precision: 0.7502 - val_loss: 0.6541 - val_precision: 0.7514\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.6532 - precision: 0.7531 - val_loss: 0.6490 - val_precision: 0.7693\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.6514 - precision: 0.7539 - val_loss: 0.6470 - val_precision: 0.7687\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa07b354d60>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}