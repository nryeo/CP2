{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stebechoi/CP2/blob/YJ/FM_%EC%B5%9C%EC%A2%85.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-s3rzgNCtJW6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import heapq\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "587CQVoNlF_h",
        "outputId": "de9ecec7-49a1-4cdb-821f-140d901db754"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 불러오기"
      ],
      "metadata": {
        "id": "0DMqzeCledkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/CP2/ml-100k/'\n",
        "r_cols =['user_id','movie_id','rating','timestamp']\n",
        "ratings = pd.read_csv(data_path + 'u.data', sep='\\t',names = r_cols, encoding ='latin-1')\n",
        "item_df = pd.read_csv(data_path + 'u.item', sep='|', encoding='latin-1', header=None,\n",
        "                        names=['movie_id', 'movie_title', 'release_date', 'video_release_date',\n",
        "                               'IMDb_URL', 'unknown', 'Action', 'Adventure', 'Animation',\n",
        "                               'Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama',\n",
        "                               'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery',\n",
        "                               'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'])\n",
        "item_df = item_df.iloc[:,:2]"
      ],
      "metadata": {
        "id": "BToT_VvetSDP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 병합"
      ],
      "metadata": {
        "id": "qOWH-7HoekKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratingcount = ratings.groupby(['movie_id'])['rating'].count().reset_index().rename(columns = {'rating': 'TotalRatingCount'})\n",
        "n_ratings = pd.merge(ratings,ratingcount,how='left',on='movie_id')\n",
        "n_ratings = pd.merge(n_ratings,item_df,how='left', on='movie_id')\n",
        "\n",
        "df_movie_100 = n_ratings[n_ratings['TotalRatingCount']>=100]\n",
        "df_movie_100 = df_movie_100.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "AO0JuoHGqiyx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_100 = df_movie_100.drop(['user_id','rating','timestamp','TotalRatingCount'],axis=1)"
      ],
      "metadata": {
        "id": "OlNw8ExRztfv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## X vector 생성"
      ],
      "metadata": {
        "id": "rg9pqUu4eonp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#user encoding\n",
        "user_dict = {}\n",
        "for i in set(df_movie_100['user_id']):\n",
        "  user_dict[i] = len(user_dict)\n",
        "n_user = len(user_dict)\n",
        "\n",
        "#item encoding\n",
        "item_dict = {}\n",
        "start_point = n_user\n",
        "for i in set(df_movie_100['movie_id']):\n",
        "    item_dict[i] = start_point + len(item_dict)\n",
        "n_item = len(item_dict)\n",
        "start_point += n_item\n",
        "num_x = start_point\n",
        "df_movie_100 = shuffle(df_movie_100, random_state=1)\n",
        "\n",
        "#generate x data\n",
        "#[user_index, movie_index], [user]\n",
        "data = []\n",
        "y = []\n",
        "w0 = np.mean(df_movie_100['rating'])\n",
        "for i in range(len(df_movie_100)):\n",
        "  case = df_movie_100.iloc[i]\n",
        "  x_index = []\n",
        "  x_value = []\n",
        "  x_index.append(user_dict[case['user_id']])\n",
        "  x_value.append(1)\n",
        "  x_index.append(item_dict[case['movie_id']])\n",
        "  x_value.append(1)\n",
        "  data.append([x_index, x_value])\n",
        "  y.append(case['rating'] - w0)\n"
      ],
      "metadata": {
        "id": "wPCFYlv9tvEw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FM 모델"
      ],
      "metadata": {
        "id": "OIVMYLV9gCuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RMSE(y_true, y_pred):\n",
        "  return np.sqrt(np.mean((np.array(y_true) - np.array(y_pred))**2))\n",
        "\n",
        "class FM():\n",
        "  def __init__(self, N, K, data, y, alpha, beta, train_ratio=0.75, iterations=100, tolerance=0.005, l2_reg=True, verbose=True):\n",
        "    self.K = K    #no of latent factors\n",
        "    self.N = N    #no of x variables\n",
        "    self.n_cases = len(data)\n",
        "    self.alpha = alpha\n",
        "    self.beta = beta\n",
        "    self.iterations = iterations\n",
        "    self.l2_reg = l2_reg\n",
        "    self.tolerance = tolerance\n",
        "    self.verbose = verbose\n",
        "\n",
        "    # w 초기화  변수의 편향\n",
        "    self.w = np.random.normal(scale=1./self.N, size=(self.N))\n",
        "    # v 초기화  잠재요인행렬\n",
        "    self.v = np.random.normal(scale=1./self.K, size=(self.N, self.K))\n",
        "    #Train/ Test 분리\n",
        "    cutoff = int(train_ratio* len(data))\n",
        "    self.train_x = data[:cutoff]\n",
        "    self.test_x = data[cutoff:]\n",
        "    self.train_y = y[:cutoff]\n",
        "    self.test_y = y[cutoff:]\n",
        "\n",
        "  #학습 함수\n",
        "  def test(self):\n",
        "    best_RMSE = 10000\n",
        "    best_iteration = 0\n",
        "    training_process = []  #학습과정을 기록\n",
        "    for i in range(self.iterations):\n",
        "      rmse1 = self.sgd(self.train_x, self.train_y)\n",
        "      rmse2 = self.test_rmse(self.test_x, self.test_y)\n",
        "      training_process.append((i,rmse1,rmse2))\n",
        "      if self.verbose:\n",
        "        if(i+1) % 10 == 0:\n",
        "          print(\"Iteration: %d ; Train RMSE = %.6f ; Test RMSE = %.6f\" % (i+1, rmse1, rmse2))\n",
        "      #기존의 RMSE보다 향상되었으면 새로운 RMSE 와 iteration기록\n",
        "      if best_RMSE> rmse2:\n",
        "        best_RMSE = rmse2\n",
        "        best_iteration = i\n",
        "      elif(rmse2 - best_RMSE) >self.tolerance:\n",
        "\n",
        "        break\n",
        "    print(best_iteration, best_RMSE)\n",
        "    return training_process\n",
        "  #sgd 실행\n",
        "  def sgd(self, x_data,  y_data):\n",
        "    y_pred = []\n",
        "    for data, y in zip(x_data, y_data):\n",
        "      x_idx = data[0]\n",
        "      x_0 = np.array(data[1])\n",
        "      x_1 = x_0.reshape(-1,1)\n",
        "\n",
        "      bias_score = np.sum(self.w[x_idx]*x_0)\n",
        "      vx = self.v[x_idx]*(x_1)\n",
        "      sum_vx = np.sum(vx, axis=0)\n",
        "      sum_vx_2 = np.sum(vx*vx, axis=0)\n",
        "      latent_score = 0.5*np.sum(np.square(sum_vx) - sum_vx_2)\n",
        "\n",
        "      y_hat = bias_score + latent_score\n",
        "      y_pred.append(y_hat)\n",
        "      error = y - y_hat\n",
        "\n",
        "      if self.l2_reg:\n",
        "        self.w[x_idx] += error*self.alpha*(x_0 - self.beta*self.w[x_idx])\n",
        "        self.v[x_idx] += error*self.alpha*((x_1)*sum(vx) - (vx*x_1) - self.beta*self.v[x_idx])\n",
        "      else:\n",
        "        self.w[x_idx] += error*self.alpha*x_0\n",
        "        self.v[x_idx] += error*self.alpha*((x_1)*sum(vx) - (vx*x_1))\n",
        "    return RMSE(y_data, y_pred)\n",
        "\n",
        "  def test_rmse(self, x_data, y_data):\n",
        "    y_pred =[]\n",
        "    for data, y in zip(x_data, y_data):\n",
        "      y_hat = self.predict(data[0], data[1])\n",
        "      y_pred.append(y_hat)\n",
        "    return RMSE(y_data, y_pred)\n",
        "  \n",
        "  def predict(self, idx, x):\n",
        "    x_0 = np.array(x)\n",
        "    x_1 = x_0.reshape(-1,1)\n",
        "    bias_score =  np.sum(self.w[idx]*x_0)\n",
        "\n",
        "    vx = self.v[idx]*(x_1)\n",
        "    sum_vx = np.sum(vx, axis=0)\n",
        "    sum_vx_2 = np.sum(vx*vx,axis=0)\n",
        "    latent_score = 0.5*np.sum(np.square(sum_vx)-sum_vx_2)\n",
        "\n",
        "    y_hat = bias_score + latent_score\n",
        "    return y_hat\n"
      ],
      "metadata": {
        "id": "PqesN8BAuQQ5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K=200\n",
        "fm1 = FM(num_x, K, data, y, alpha=0.001, beta=0.05, train_ratio=0.75, iterations=300, tolerance=0.0005, l2_reg=True, verbose=True)\n",
        "result = fm1.test()"
      ],
      "metadata": {
        "id": "OPZk9aGCgafI",
        "outputId": "2227f459-5aaf-415b-a815-f16fe51db416",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 10 ; Train RMSE = 0.936163 ; Test RMSE = 0.940418\n",
            "Iteration: 20 ; Train RMSE = 0.915684 ; Test RMSE = 0.925847\n",
            "Iteration: 30 ; Train RMSE = 0.909258 ; Test RMSE = 0.922113\n",
            "Iteration: 40 ; Train RMSE = 0.906178 ; Test RMSE = 0.920672\n",
            "Iteration: 50 ; Train RMSE = 0.904315 ; Test RMSE = 0.919993\n",
            "Iteration: 60 ; Train RMSE = 0.902985 ; Test RMSE = 0.919640\n",
            "Iteration: 70 ; Train RMSE = 0.901871 ; Test RMSE = 0.919428\n",
            "Iteration: 80 ; Train RMSE = 0.900762 ; Test RMSE = 0.919252\n",
            "Iteration: 90 ; Train RMSE = 0.899459 ; Test RMSE = 0.919021\n",
            "Iteration: 100 ; Train RMSE = 0.897713 ; Test RMSE = 0.918621\n",
            "Iteration: 110 ; Train RMSE = 0.895168 ; Test RMSE = 0.917889\n",
            "Iteration: 120 ; Train RMSE = 0.891302 ; Test RMSE = 0.916576\n",
            "Iteration: 130 ; Train RMSE = 0.885416 ; Test RMSE = 0.914363\n",
            "Iteration: 140 ; Train RMSE = 0.876766 ; Test RMSE = 0.910987\n",
            "Iteration: 150 ; Train RMSE = 0.864927 ; Test RMSE = 0.906510\n",
            "Iteration: 160 ; Train RMSE = 0.850089 ; Test RMSE = 0.901504\n",
            "Iteration: 170 ; Train RMSE = 0.832734 ; Test RMSE = 0.896712\n",
            "Iteration: 180 ; Train RMSE = 0.813006 ; Test RMSE = 0.892523\n",
            "Iteration: 190 ; Train RMSE = 0.790730 ; Test RMSE = 0.888974\n",
            "Iteration: 200 ; Train RMSE = 0.765880 ; Test RMSE = 0.886079\n",
            "Iteration: 210 ; Train RMSE = 0.738752 ; Test RMSE = 0.883934\n",
            "Iteration: 220 ; Train RMSE = 0.709852 ; Test RMSE = 0.882659\n",
            "Iteration: 230 ; Train RMSE = 0.679728 ; Test RMSE = 0.882318\n",
            "Iteration: 240 ; Train RMSE = 0.648889 ; Test RMSE = 0.882905\n",
            "228 0.8823091708771533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def recommendations(user_id, k):\n",
        "  user_predictions = []\n",
        "  user_index = user_dict[user_id]\n",
        "  for movie_id, item_index in item_dict.items():\n",
        "      x_index = [user_index, item_index]\n",
        "      x_value = [1, 1]\n",
        "      predicted_rating = fm1.predict(x_index, x_value)\n",
        "      user_predictions.append((movie_id, predicted_rating))\n",
        "  \n",
        "  top_k_recommendations = heapq.nlargest(k, user_predictions, key=lambda x: x[1])\n",
        "  id_list = [movie_id for movie_id, _ in top_k_recommendations]\n",
        "  rec_list=[]\n",
        "  for i,r in movie_100.iterrows():\n",
        "    if i in id_list:\n",
        "      rec_list.append(r['movie_title'])\n",
        "  return rec_list"
      ],
      "metadata": {
        "id": "EPMgmmgjlN6N"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommendations(1,5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie49Hk_4wz30",
        "outputId": "0f01730d-f2a4-4a6f-b9c3-58ac77e1b2d6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Star Trek III: The Search for Spock (1984)',\n",
              " 'Real Genius (1985)',\n",
              " 'Kolya (1996)',\n",
              " 'Silence of the Lambs, The (1991)',\n",
              " 'Ghost (1990)']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_k_recommendations(user_id, k):\n",
        "    user_predictions = []\n",
        "    user_index = user_dict[user_id]\n",
        "    for movie_id, item_index in item_dict.items():\n",
        "        x_index = [user_index, item_index]\n",
        "        x_value = [1, 1]\n",
        "        predicted_rating = fm1.predict(x_index, x_value)\n",
        "        user_predictions.append((movie_id, predicted_rating))\n",
        "    \n",
        "    top_k_recommendations = heapq.nlargest(k, user_predictions, key=lambda x: x[1])\n",
        "    return [movie_id for movie_id, _ in top_k_recommendations]"
      ],
      "metadata": {
        "id": "3mV1Qoxp28VR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_at_k(user_id, k):\n",
        "    user_ratings = df_movie_100[df_movie_100['user_id'] == user_id]\n",
        "    test_ratings = user_ratings.sample(frac=0.2, random_state=1)\n",
        "    top_k_recommendations = get_top_k_recommendations(user_id, k)\n",
        "\n",
        "    relevant_items = set(test_ratings[test_ratings['rating'] >= 4]['movie_id'].values)\n",
        "    recommended_relevant_items = [movie_id for movie_id in top_k_recommendations if movie_id in relevant_items]\n",
        "    \n",
        "    precision = len(recommended_relevant_items) / k\n",
        "    return precision\n",
        "\n",
        "\n",
        "k = 10\n",
        "all_user_precision_at_k = [precision_at_k(user_id, k) for user_id in user_dict.keys()]\n",
        "mean_precision_at_k = np.mean(all_user_precision_at_k)\n",
        "print(\"Precision@{}: {:.4f}\".format(k, mean_precision_at_k))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAktaTfV0-Vv",
        "outputId": "e93f61d7-41ad-46d3-aca9-f9b92e8710f7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision@10: 0.0738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def recall_at_k(user_id, k):\n",
        "    user_ratings = df_movie_100[df_movie_100['user_id'] == user_id]\n",
        "    test_ratings = user_ratings.sample(frac=0.2, random_state=1)\n",
        "    top_k_recommendations = get_top_k_recommendations(user_id, k)\n",
        "\n",
        "    relevant_items = set(test_ratings[test_ratings['rating'] >= 4]['movie_id'].values)\n",
        "    recommended_relevant_items = [movie_id for movie_id in top_k_recommendations if movie_id in relevant_items]\n",
        "    \n",
        "    recall = len(recommended_relevant_items) / len(relevant_items) if relevant_items else 0\n",
        "    return recall\n",
        "\n",
        "\n",
        "k = 10\n",
        "all_user_recall_at_k = [recall_at_k(user_id, k) for user_id in user_dict.keys()]\n",
        "mean_recall_at_k = np.mean(all_user_recall_at_k)\n",
        "print(\"Recall@{}: {:.4f}\".format(k, mean_recall_at_k))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWubxAuH3Uqh",
        "outputId": "a54e1376-6f93-4b25-8aef-09628814fff1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall@10: 0.0913\n"
          ]
        }
      ]
    }
  ]
}